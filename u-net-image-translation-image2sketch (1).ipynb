{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Image Translation: Image to Sketch using U-NET Architecture\nWe will be using U-NET architecture to translate facial picture to its sketch using CUHK facial dataset.\\\n","metadata":{"execution":{"iopub.status.busy":"2023-06-16T21:17:11.176826Z","iopub.execute_input":"2023-06-16T21:17:11.177309Z","iopub.status.idle":"2023-06-16T21:17:19.689956Z","shell.execute_reply.started":"2023-06-16T21:17:11.17727Z","shell.execute_reply":"2023-06-16T21:17:19.688968Z"}}},{"cell_type":"markdown","source":"![Title Image](https://www.researchgate.net/profile/Vinayakumar_Ravi/publication/347776755/figure/download/fig1/AS:976145776377858@1609742697583/Convolutional-Neural-Network-architecture.png)\\\nNote: This image has nothing to do with the model being trained :)\nBase Reference: https://www.kaggle.com/code/theblackmamba31/photo-to-sketch-using-autoencoder","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport keras\nfrom keras.utils import img_to_array\nimport keras.applications.resnet as resnet\nimport keras.backend as K\nimport tqdm\nimport cv2","metadata":{"execution":{"iopub.status.busy":"2023-06-17T06:44:05.053955Z","iopub.execute_input":"2023-06-17T06:44:05.054319Z","iopub.status.idle":"2023-06-17T06:44:13.821006Z","shell.execute_reply.started":"2023-06-17T06:44:05.054279Z","shell.execute_reply":"2023-06-17T06:44:13.820041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## GPU Connectivity\nTo speed up the training, we will be using GPU. While this code is unnecessary in kaggle, but this code can be helpful when using your offline gpu.\\\nMake sure to select the accelator as one of the gpus, preferably GPU P100 while running on kaggle.","metadata":{}},{"cell_type":"code","source":"# Configure TensorFlow to use GPU\ngpus = tf.config.experimental.list_physical_devices('GPU')\nprint(gpus)\nif gpus:\n    try:\n        # Restrict TensorFlow to only use the first GPU\n        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n        tf.config.experimental.set_memory_growth(gpus[0], True)\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n    except RuntimeError as e:\n        # Visible devices must be set before GPUs have been initialized\n        print(e)","metadata":{"execution":{"iopub.status.busy":"2023-06-17T06:44:13.825564Z","iopub.execute_input":"2023-06-17T06:44:13.826159Z","iopub.status.idle":"2023-06-17T06:44:16.192656Z","shell.execute_reply.started":"2023-06-17T06:44:13.826133Z","shell.execute_reply":"2023-06-17T06:44:16.191646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"","metadata":{"execution":{"iopub.status.busy":"2023-06-17T06:44:16.194206Z","iopub.execute_input":"2023-06-17T06:44:16.194817Z","iopub.status.idle":"2023-06-17T06:44:16.202536Z","shell.execute_reply.started":"2023-06-17T06:44:16.194791Z","shell.execute_reply":"2023-06-17T06:44:16.201539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Importing Dataset\nHere, we are importing images, and storing them collectively.","metadata":{}},{"cell_type":"code","source":"import os\nimage_path = '/kaggle/input/cuhk-face-sketch-database-cufs/photos' #'data/photos'\nsketches_path = '/kaggle/input/cuhk-face-sketch-database-cufs/sketches' #'data/sketches'\n\nimage_names = []\nsketch_names = []\n\nfor i in os.listdir(image_path):\n    image_names.append(i)\n\nfor i in os.listdir(sketches_path):\n    sketch_names.append(i)\nimage_names = sorted(image_names)\nsketch_names = sorted(sketch_names)\nprint(\"Images Names\", image_names)\nprint(\"Sketch Names\", sketch_names)","metadata":{"execution":{"iopub.status.busy":"2023-06-17T06:44:16.203801Z","iopub.execute_input":"2023-06-17T06:44:16.204171Z","iopub.status.idle":"2023-06-17T06:44:16.477654Z","shell.execute_reply.started":"2023-06-17T06:44:16.204141Z","shell.execute_reply":"2023-06-17T06:44:16.47665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Augmentation\nData augmentation is a very important step. As our dataset is pretty small, we will augment it to create new images by flipping and rotating them.\\\nThis will give us around 1504 images for training and testing.\\\nSize is kept to 256, if working on offline GPU, and GPU memory is not greater, suggested to lower the size to 128 or around 100.","metadata":{}},{"cell_type":"code","source":"img_array = []\nsketch_array = []\nsize = 256\n\nfor img_name in tqdm.tqdm(image_names):\n    img = image_path +  \"/\" + img_name\n    img = cv2.imread(img, 1)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, (size, size))\n    img = img.astype('float32')/255\n    # Appending images to img_array\n    img_array.append(img_to_array(img))\n\n    ## Augmentation\n    # Horizontal Flip on Upright Image\n    img1 = cv2.flip(img, 1)\n    # Vertical Flip to Downright Image\n    img2 = cv2.flip(img, -1)\n    # Horizontal Flip on Downright Image\n    img3 = cv2.flip(img2, 1)\n    # Clockwise Rotate 90 degrees\n    img4 = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n    # Clockwise Rotate 270 degrees\n    img5 = cv2.flip(img4, 1)\n    # Counter Clockwise Rotate 90 degrees\n    img6 = cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)\n    # Counter Clockwise Rotate 270 degrees\n    img7 = cv2.flip(img, 1)\n    img_array.append(img_to_array(img1))\n    img_array.append(img_to_array(img2))\n    img_array.append(img_to_array(img3))\n    img_array.append(img_to_array(img4))\n    img_array.append(img_to_array(img5))\n    img_array.append(img_to_array(img6))\n    img_array.append(img_to_array(img7))\n\nfor img_name in tqdm.tqdm(sketch_names):\n    img = sketches_path +  \"/\" + img_name\n    img = cv2.imread(img, 1)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, (size, size))\n    img = img.astype('float32')/255\n    # Appending images to img_array\n    sketch_array.append(img_to_array(img))\n\n    ## Augmentation\n    # Horizontal Flip on Upright Image\n    img1 = cv2.flip(img, 1)\n    # Vertical Flip to Downright Image\n    img2 = cv2.flip(img, -1)\n    # Horizontal Flip on Downright Image\n    img3 = cv2.flip(img2, 1)\n    # Clockwise Rotate 90 degrees\n    img4 = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n    # Clockwise Rotate 270 degrees\n    img5 = cv2.flip(img4, 1)\n    # Counter Clockwise Rotate 90 degrees\n    img6 = cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)\n    # Counter Clockwise Rotate 270 degrees\n    img7 = cv2.flip(img, 1)\n    sketch_array.append(img_to_array(img1))\n    sketch_array.append(img_to_array(img2))\n    sketch_array.append(img_to_array(img3))\n    sketch_array.append(img_to_array(img4))\n    sketch_array.append(img_to_array(img5))\n    sketch_array.append(img_to_array(img6))\n    sketch_array.append(img_to_array(img7))","metadata":{"execution":{"iopub.status.busy":"2023-06-17T06:44:16.481293Z","iopub.execute_input":"2023-06-17T06:44:16.481587Z","iopub.status.idle":"2023-06-17T06:44:21.504329Z","shell.execute_reply.started":"2023-06-17T06:44:16.481562Z","shell.execute_reply":"2023-06-17T06:44:21.502855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"The number of total Images is: {len(sketch_array)}\")","metadata":{"execution":{"iopub.status.busy":"2023-06-17T06:44:21.505722Z","iopub.execute_input":"2023-06-17T06:44:21.506287Z","iopub.status.idle":"2023-06-17T06:44:21.512811Z","shell.execute_reply.started":"2023-06-17T06:44:21.506253Z","shell.execute_reply":"2023-06-17T06:44:21.511661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Splitting the Data: Train & Valid\nHere, we will split our dataset into training and validation i.e. testing datasets.","metadata":{}},{"cell_type":"code","source":"train_sketch_images = sketch_array[:1400]\ntrain_original_images = img_array[:1400]\ntest_sketch_images = sketch_array[1400:]\ntest_original_images = img_array[1400:]\ntrain_sketch_images = np.reshape(train_sketch_images, (len(train_sketch_images), int(size), int(size), 3))\ntrain_original_images = np.reshape(train_original_images, (len(train_original_images), int(size), int(size), 3))\nprint('Train sketch images size: ', train_sketch_images.shape)\ntest_sketch_images = np.reshape(test_sketch_images, (len(test_sketch_images), int(size), int(size), 3))\ntest_original_images = np.reshape(test_original_images, (len(test_original_images), int(size), int(size), 3))\nprint('Test sketch images size: ', test_sketch_images.shape)","metadata":{"execution":{"iopub.status.busy":"2023-06-17T06:44:21.514468Z","iopub.execute_input":"2023-06-17T06:44:21.514886Z","iopub.status.idle":"2023-06-17T06:44:22.218756Z","shell.execute_reply.started":"2023-06-17T06:44:21.514845Z","shell.execute_reply":"2023-06-17T06:44:22.217649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modelling: Defining Model Structure\nThe most important step in our notebook / tutorial. Here we are defining the structure of our model which resmbles or is imitated as a U-NET architecture.","metadata":{}},{"cell_type":"code","source":"from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate\nfrom keras.models import Model\n\n# Define the U-Net architecture\ndef unet_model(input_shape):\n    # Input Layer\n    inputs = Input(input_shape)\n\n    # Encoder\n    conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)\n    conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n\n    conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)\n    conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n\n    conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)\n    conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n\n    conv4 = Conv2D(512, 3, activation='relu', padding='same')(pool3)\n    conv4 = Conv2D(512, 3, activation='relu', padding='same')(conv4)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n\n    conv5 = Conv2D(1024, 3, activation='relu', padding='same')(pool4)\n    conv5 = Conv2D(1024, 3, activation='relu', padding='same')(conv5)\n\n    # Decoder\n    up6 = Conv2DTranspose(512, 2, strides=(2, 2), padding='same')(conv5)\n    up6 = concatenate([up6, conv4])\n    conv6 = Conv2D(512, 3, activation='relu', padding='same')(up6)\n    conv6 = Conv2D(512, 3, activation='relu', padding='same')(conv6)\n\n    up7 = Conv2DTranspose(256, 2, strides=(2, 2), padding='same')(conv6)\n    up7 = concatenate([up7, conv3])\n    conv7 = Conv2D(256, 3, activation='relu', padding='same')(up7)\n    conv7 = Conv2D(256, 3, activation='relu', padding='same')(conv7)\n\n    up8 = Conv2DTranspose(128, 2, strides=(2, 2), padding='same')(conv7)\n    up8 = concatenate([up8, conv2])\n    conv8 = Conv2D(128, 3, activation='relu', padding='same')(up8)\n    conv8 = Conv2D(128, 3, activation='relu', padding='same')(conv8)\n\n    up9 = Conv2DTranspose(64, 2, strides=(2, 2), padding='same')(conv8)\n    up9 = concatenate([up9, conv1])\n    conv9 = Conv2D(64, 3, activation='relu', padding='same')(up9)\n    conv9 = Conv2D(64, 3, activation='relu', padding='same')(conv9)\n\n    # Output layer\n    output = Conv2D(3, 1, activation='linear', padding='same')(conv9)\n\n    # Create the model\n    model = Model(inputs=inputs, outputs=output)\n\n    return model\n\n# Define the input shape\ninput_shape = (int(size), int(size), 3)\n\n# Create the U-Net model\nmodel = unet_model(input_shape)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='mse')\n\n# Print the model summary\nfinal_model = model\nfinal_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-06-17T06:44:22.221929Z","iopub.execute_input":"2023-06-17T06:44:22.222206Z","iopub.status.idle":"2023-06-17T06:44:22.95052Z","shell.execute_reply.started":"2023-06-17T06:44:22.222181Z","shell.execute_reply":"2023-06-17T06:44:22.949727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modelling: Training the model\nWe are going to use 'val_loss' i.e. validation loss to incorporate early stopping if possible, it is an important step if you don't want to waste time upon very minor or no growth in precision of the model.\\\nYou can introduce batch_size parameter if you are facing memory exceeding errors, keep it as small as possible, but keep it as an integer.","metadata":{}},{"cell_type":"code","source":"from keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\nfinal_model.fit(train_original_images, train_sketch_images, epochs=100,\n                validation_data=(test_original_images, test_sketch_images),\n                verbose=1, callbacks=[early_stopping])","metadata":{"execution":{"iopub.status.busy":"2023-06-17T06:44:22.951553Z","iopub.execute_input":"2023-06-17T06:44:22.951903Z","iopub.status.idle":"2023-06-17T06:54:07.430901Z","shell.execute_reply.started":"2023-06-17T06:44:22.951869Z","shell.execute_reply":"2023-06-17T06:54:07.429855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Saving the model","metadata":{}},{"cell_type":"code","source":"# Saving the model\nfinal_model.save_weights('unet_weights_kaggle')\nfinal_model.save('unet_sketch_gen_kaggle.h5')","metadata":{"execution":{"iopub.status.busy":"2023-06-17T06:54:07.438432Z","iopub.execute_input":"2023-06-17T06:54:07.438931Z","iopub.status.idle":"2023-06-17T06:54:09.979391Z","shell.execute_reply.started":"2023-06-17T06:54:07.438899Z","shell.execute_reply":"2023-06-17T06:54:09.97833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing the Predictions","metadata":{}},{"cell_type":"code","source":"def show_images(real, sketch, predicted):\n    plt.figure(figsize=(12, 12))\n    plt.subplot(1, 3, 1)\n    plt.title(\"Image\", fontsize=15, color='Lime')\n    plt.imshow(real)\n    plt.subplot(1, 3, 2)\n    plt.title(\"sketch\", fontsize=15, color='Blue')\n    plt.imshow(sketch)\n    plt.subplot(1, 3, 3)\n    plt.title(\"Predicted\", fontsize=15, color='gold')\n    plt.imshow(predicted)\n\n\nls = [i for i in range(0, 10, 8)]\nfor i in ls:\n    predicted = np.clip(final_model.predict(test_original_images[i].reshape(1, size, size, 3)), 0.0, 1.0).reshape(size, size,                                                               3)\n    show_images(test_original_images[i], test_sketch_images[i], predicted)","metadata":{"execution":{"iopub.status.busy":"2023-06-17T06:54:09.984311Z","iopub.execute_input":"2023-06-17T06:54:09.984591Z","iopub.status.idle":"2023-06-17T06:54:12.616371Z","shell.execute_reply.started":"2023-06-17T06:54:09.984567Z","shell.execute_reply":"2023-06-17T06:54:12.615471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"prediction_on_test_data = final_model.evaluate(test_original_images, test_sketch_images)\nprint(\"Loss: \", prediction_on_test_data)\nprint(\"Accuracy: \", 100 - np.round(prediction_on_test_data * 100,3))","metadata":{"execution":{"iopub.status.busy":"2023-06-17T06:54:12.617757Z","iopub.execute_input":"2023-06-17T06:54:12.618651Z","iopub.status.idle":"2023-06-17T06:54:13.545078Z","shell.execute_reply.started":"2023-06-17T06:54:12.618619Z","shell.execute_reply":"2023-06-17T06:54:13.544195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}